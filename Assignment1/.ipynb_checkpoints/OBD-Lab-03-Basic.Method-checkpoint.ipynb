{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>  MATH11146: Modern Optimization Methods for Big Data Problems </center>\n",
    "\n",
    "<center> University of Edinburgh</center>\n",
    "\n",
    "<center>Lecturer: Peter Richtarik</center>\n",
    "\n",
    "<center>Tutors: Sona Galovicova, Filip Hanzely and Nicolas Loizou</center>\n",
    "\n",
    "##  <center>Lab 3: Randomized Iterative Methods for Linear Systems</center>\n",
    "<center>(C) Dominik Csiba and Peter Richtarik </center>\n",
    "<center> 23.1.2017 </center>\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "\n",
    "Consider the problem of solving a consistent linear system:\n",
    "\n",
    "$$Ax = b,$$\n",
    "\n",
    "where $A\\in \\mathbb{R}^{m\\times n}$ and $b\\in \\mathbb{R}^m$. Recall that the <b>basic method</b> with stepsize $\\omega>0$ performs iterations of the form\n",
    "\n",
    "$$ x_{k+1} \\leftarrow x_{k} - \\omega B^{-1} A^\\top S_k (S_k^\\top A B^{-1} A^\\top S_k)^\\dagger S_k^\\top (Ax_k-b), $$\n",
    "\n",
    "where \n",
    "- $B$ is a fixed $n\\times n$ positive definite matrix, and\n",
    "- $S_k$ is drawn in an i.i.d. fashion from a fixed distribution $\\cal D$.\n",
    "\n",
    "Recall that the method can be interpreted in several ways (e.g., stochastic gradient descent, stochastic Newton descent, stochastic fixed point method with relaxation, stochastic projection method).\n",
    "\n",
    "### 1.1 Convergence\n",
    "\n",
    "We shall prove in the course that, under some assumptions (assuming that $E[H]$ is positive definite is sufficient), the basic method with unit stepsize ($\\omega=1$) converges. In particular, let $x_*$ be the $B$-projection of the starting point $x_0$ onto ${\\cal L} = \\{x\\;:\\; Ax=b\\}$. Then\n",
    "\n",
    "$$ E\\left[\\|x_k-x_*\\|_B^2\\right] \\leq \\rho^k \\|x_0-x_*\\|_B^2,$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\rho = 1 - \\lambda_{\\min}^+(W), \\qquad W = B^{-1/2}A^\\top E\\left[H\\right] A B^{-1/2}.$$\n",
    "\n",
    "By $\\lambda_{\\min}^+(W)$ we mean the smallest nonzero eigenvalue of $W$.\n",
    "\n",
    "In this lab, the <b>basic method</b> with unit stepsize was coded up for you. Play with the code and try to see what it does. There will be some exercices at the end. Do at least one - there will be no time to do (much) more than that in the lab. However, you are strongly encouraged to work on some of the other exercises at home.\n",
    "\n",
    "### 1.2 Randomized Kaczmarz (RK) method\n",
    "\n",
    "Let $e_1,e_2,\\dots,e_m$ be the standard basis vectors in $\\mathbb{R}^m$. That is, $e_i = (0,\\dots,0,1,0,\\dots,0)\\in \\mathbb{R}^m$, where 1 is in position $i$. If $\\cal D$ is given by $S_k = e_i$ with probability $p_i>0$ for all $i=1,2,\\dots,m$, and if $B$ is chosen to be equal to the identity matrix, then the basic method reduces to an algorithm that is known under the name <b>randomized Kaczmarz method</b> [1]. \n",
    "\n",
    "The standard randomized Kaczmarz method [1] uses the probabilities\n",
    "\n",
    "$$p_i = \\frac{\\|A_{i:}^\\top\\|_2^2}{\\sum_{j=1}^m \\|A_{j:}^\\top\\|_2^2} = \\frac{\\|A_{i:}^\\top\\|_2^2}{\\|A\\|_F^2},$$\n",
    "\n",
    "where $\\|A\\|_F$ denotes the <b>Frobenius norm</b> of $A$. Note that the Frobenius norm of $A$ is the standard Euclidean norm of the  vector obtained from the entries of $A$ by stacking them up into a single  vector in $\\mathbb{R}^{mn}$.\n",
    "\n",
    "Further, note that \n",
    "\n",
    "$$W = A^\\top E\\left[H\\right] A $$\n",
    "\n",
    "where for the standard RK method we have\n",
    "$$\\begin{eqnarray}E[H] &=& E [S (S^\\top A  A^\\top S)^\\dagger S^\\top] \n",
    "\\\\\n",
    "&=& \\sum_{i=1}^m p_i e_i (e_i^\\top A  A^\\top e_i)^\\dagger e_i^\\top\\\\\n",
    "&=& \\sum_{i=1}^m \\frac{p_i}{\\|A_{i:}^\\top\\|_2^2} e_i e_i^\\top \\\\\n",
    "&=& \\frac{1}{\\|A\\|^2_F} \\sum_{i=1}^m e_i e_i^\\top \\\\\n",
    "&=& \\frac{I}{\\|A\\|^2_F},\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "where $I$ denotes the $m\\times m$ identity matrix. Therefore, the convergence rate of the standard RK method is given by\n",
    "\n",
    "$$\\rho = 1 - \\lambda_{\\min}^+(W) = 1 - \\lambda_{\\min}^+\\left(\\frac{A^\\top A}{\\|A\\|_F^2}\\right) = 1 - \\frac{\\lambda_{\\min}^+\\left(A^\\top A\\right)}{\\|A\\|_F^2}.$$\n",
    "\n",
    "\n",
    "### 1.3 Bibliography Remarks\n",
    "\n",
    "The randomized Kaczmarz method was developed in [1]. The lectures on linear systems are largely based on [4]. You may wish to read [2] to get more insights about the basic method, beyond what we have covered in class. A duality theory was developed in [3].\n",
    "\n",
    "[1] Thomas Strohmer and Roman Vershynin. A randomized Kaczmarz algorithm with exponential convergence, <i>Journal of Fourier Analysis and Applications,</i> 15:262, 2009.\n",
    "\n",
    "[2] Robert Gower and Peter Richtarik. Randomized iterative methods for linear systems, <i>SIAM Journal on Matrix Analysis and Applications</i> 36(4):1660-1690,  2015.\n",
    "\n",
    "[3] Robert Gower and Peter Richtarik. Stochastic dual ascent for solving linear systems, <i>arXiv:1512.06890</i>, 2015.\n",
    "\n",
    "[4] Peter Richtarik and Martin Takac. Stochastic reformulations of linear systems and efficient randomized algorithms, <i>Manuscript,</i> 2016.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate data\n",
    "\n",
    "We first generate an $m\\times n$ matrix $A$, then a random vector $x^*$ ($\\verb\"x_star\"$) and finally, set $b = A x^*$. This way we will know the system we have generated is consistent (i.e., that it has a solution).\n",
    "\n",
    "## 2.1 Silly synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000-element Array{Float64,1}:\n",
       "   0.334528\n",
       "   2.18282 \n",
       "   6.6722  \n",
       "  10.0642  \n",
       "   4.27898 \n",
       "   1.46443 \n",
       "   1.01054 \n",
       "  -0.908806\n",
       "  -8.40474 \n",
       "  -3.98806 \n",
       "  -5.25256 \n",
       "   3.95305 \n",
       "  -7.94644 \n",
       "   ⋮       \n",
       "   2.05124 \n",
       "  -0.194267\n",
       "  -6.17476 \n",
       "  -3.55684 \n",
       " -10.759   \n",
       "  16.0267  \n",
       "   7.1659  \n",
       " -14.1444  \n",
       "  -3.0117  \n",
       "  -1.17877 \n",
       "  -4.56065 \n",
       "   5.16672 "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srand(1) # set random seed to 1\n",
    "\n",
    "m = 1000\n",
    "n = 50\n",
    "\n",
    "# Generate random matrix A\n",
    "\n",
    "A = randn(m,n) # A has random standard normal entries. For uniform entries on [0,1], use rand() \n",
    "x_star = randn(n)\n",
    "b = A*x_star\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Smarter synthetic data (good for randomized Kaczmarz)\n",
    "\n",
    "We now generate a matrix $A\\in \\mathbb{R}^{m\\times n}$ such that the randomized Kaczmarz rate \n",
    "\n",
    "$$\\rho =1 - \\frac{\\lambda_{\\min}^+(A^\\top A)}{\\|A\\|_F^2} = 1 - \\frac{\\lambda_{\\min}^+(A^\\top A)}{Tr(A^\\top A)}$$ \n",
    "\n",
    "is under control. Specifically, we shall select the eigenvalues of $A^\\top A$ first, and then construct a random matrix $A\\in \\mathbb{R}^{m\\times n}$ whose spectrum is fixed this way. The trick is to assemble $A$ via its SVD (singular value decomposition):  \n",
    "\n",
    "$$A = U D V^\\top,$$ \n",
    "\n",
    "where $U\\in \\mathbb{R}^{m\\times m}$ and $V\\in \\mathbb{R}^{n \\times n}$ are orthonormal matrices and $D\\in \\mathbb{R}^{m\\times n}$ is diagonal (that is, $D_{ij}=0$ for $i\\neq j$). Note that given the SVD, we have  \n",
    "\n",
    "$$A^\\top A = V D^\\top D V^\\top,$$ \n",
    "\n",
    "and hence the eigenvalues of $A^\\top A$ are $D_{ii}^2$ for $i=1,2,\\dots, \\min\\{m,n\\}$. \n",
    "\n",
    "Hence, \n",
    "\n",
    "$$\\rho = 1 - \\frac{\\min_i D_{ii}^2}{\\sum_{i} D_{ii}^2}.$$\n",
    "\n",
    "So, we first generate the eigenvalues $D_{ii}^2$, and then generate two random orthoginal matrices $U\\in \\mathbb{R}^{m\\times m}$ and $V\\in \\mathbb{R}^{n\\times n}$ by performing a QR decomposition of random matrices of appropriate sizes. After this, we simply assemble $A$ from these three components via $A = U D V^\\top$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rho = 0.9999903349900235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000-element Array{Float64,1}:\n",
       "  -0.555436\n",
       "  -1.66898 \n",
       "  20.7432  \n",
       " -15.0377  \n",
       "  -5.95265 \n",
       " -15.4779  \n",
       "   0.97849 \n",
       "  10.4555  \n",
       "  12.8273  \n",
       " -13.3945  \n",
       "   9.36338 \n",
       "  -9.7277  \n",
       "  -8.23798 \n",
       "   ⋮       \n",
       "  11.3228  \n",
       "  -9.19948 \n",
       "  15.8309  \n",
       "  12.5511  \n",
       " -14.983   \n",
       "   4.17122 \n",
       "  13.1715  \n",
       "  24.4736  \n",
       "  -8.18128 \n",
       "  -6.43113 \n",
       "  22.5173  \n",
       "  15.2025  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 1000\n",
    "n = 50\n",
    "\n",
    "D = zeros(m,n)\n",
    "\n",
    "for i=1:min(m,n)   \n",
    "  D[i,i] = 10 + rand()   \n",
    "end\n",
    "\n",
    "lambda_min = minimum(diag(D'*D))\n",
    "lambda_sum = sum(diag(D'*D))\n",
    "rho = 1- lambda_min/lambda_sum\n",
    "println(\"rho = \",rho)\n",
    "\n",
    "(U,RU) = qr(randn(m,m))\n",
    "(V,RV) = qr(randn(n,n))\n",
    "A = U*D*V'\n",
    "\n",
    "x_star = randn(n)\n",
    "b = A*x_star\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Average consensus problem\n",
    "\n",
    "Consider the following problem: We have $n$ nodes on an undirected graph, with node $i$ containing value $c_i\\in \\mathbb{R}$. We wish to develop a <b>decentralized average consensus algorithm.</b> Decentralization means that in each iteration of the algorithm, only a small number of neighboring nodes can communicate (their local information) and perform computation. By consensus we mean that we wish all nodes of the network to contain the average of the values $c_1,\\dots,c_n$ at the end of the process. The process can be iterative, in which case the values computed at each nodes will only converge to the average of the $c_i$ values, not necessarily reaching this in finite time.\n",
    "\n",
    "${\\bf Fact.}$ The optimal value of the optimization problem \n",
    "\n",
    "$$\\min_{t\\in \\mathbb{R}} \\sum_{i=1}^n (c_i-t)^2$$\n",
    "\n",
    "is  $t = \\bar{c}:= \\frac{1}{n}\\sum_{i=1}^n c_i.$\n",
    "\n",
    "Using the above fact, we observe that the optimal solution of the optimization problem\n",
    "\n",
    "$$\\min \\|y-c\\|_2^2 \\quad \\text{subject to} \\quad y_1=y_2=\\dots=y_n$$\n",
    "\n",
    "is $y = \\bar{c}e$, where $e\\in \\mathbb{R}^n$ is the vector of all ones. Under the change of variables $x = y-c$, we can equivalently write the problem in the form\n",
    "\n",
    "$$\\min \\|x\\|_2^2 \\quad \\text{subject to} \\quad x_1 + c_1 = x_2 + c_2 =\\dots=x_n + c_n.$$\n",
    "\n",
    "Finally, the problem can be written in the form \n",
    "\n",
    "$$\\min \\|x\\|_2^2 \\quad \\text{subject to} \\quad Ax = b,$$\n",
    "\n",
    "where $Ax=b$ is the linear system whose $i$th equation is $x_i + c_i = x_{i+1}+c_{i+1}$ for $i=1,2,\\dots,n-1$. That is, $A = [e_1-e_2; e_2-e_3; \\dots ; e_{n-1} - e_n]$, where $e_i$ is the $i$th unit coordinate vector, and $b = [c_2-c_1; c_3-c_2; \\dots; c_n-c_{n-1}]$. \n",
    "\n",
    "We can now directly apply the basic method (with unit stepsize) to the last problem -- as the method, when started from $x_0 = 0$, automatically finds the least-norm solution of the linear system $Ax=b$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [1.0 -1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 1.0 -1.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 1.0 -1.0 0.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 1.0 -1.0 0.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 1.0 -1.0 0.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 1.0 -1.0 0.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 1.0 -1.0 0.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 -1.0 0.0; 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 -1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10-element Array{Float64,1}:\n",
       "  4.5\n",
       "  3.5\n",
       "  2.5\n",
       "  1.5\n",
       "  0.5\n",
       " -0.5\n",
       " -1.5\n",
       " -2.5\n",
       " -3.5\n",
       " -4.5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL 1\n",
    "\n",
    "n = 10\n",
    "m = 10\n",
    "\n",
    "A = zeros(m,n)\n",
    "c = 1:n\n",
    "b = zeros(m)\n",
    "\n",
    "for i=1:n-1\n",
    "    A[i,i] = 1\n",
    "    A[i,i+1] = -1\n",
    "    b[i] = c[i+1]-c[i]\n",
    "end\n",
    "A[n,n] = 1\n",
    "A[n,1] = -1\n",
    "b[n] = c[1] - c[n]\n",
    "\n",
    "# MODEL 2\n",
    "\n",
    "n = 10\n",
    "m = 9\n",
    "\n",
    "A = zeros(m,n)\n",
    "c = 1:n\n",
    "b = zeros(m)\n",
    "\n",
    "for i=1:n-1\n",
    "    A[i,i] = 1\n",
    "    A[i,i+1] = -1\n",
    "    b[i] = c[i+1]-c[i]\n",
    "end\n",
    "\n",
    "# MODEL 2\n",
    "\n",
    "n = 10\n",
    "m = 9\n",
    "\n",
    "A = zeros(m,n)\n",
    "c = 1:n\n",
    "b = zeros(m)\n",
    "\n",
    "for i=1:n-1\n",
    "    A[i,i] = 1\n",
    "    A[i,i+1] = -1\n",
    "    b[i] = c[i+1]-c[i]\n",
    "end\n",
    "\n",
    "# PRINT THE DATA MATRIX\n",
    "\n",
    "println(\"A = \", A)\n",
    "\n",
    "# PRINT THE SOLUTION\n",
    "\n",
    "y_star = ones(n)*sum(c)/n  # averages should be\n",
    "x_star =  y_star - c       # this is what the algorithm should compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Generic Solver\n",
    "\n",
    "The two functions below implement the general version of the basic method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition iterate(Any, Any, Any, Any, Any) in module Main at In[4]:4 overwritten at In[41]:4.\n",
      "WARNING: Method definition RandomLinearSolve(Any, Any, Any, Any, Any, Any, Any) in module Main at In[4]:12 overwritten at In[41]:12.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomLinearSolve (generic function with 1 method)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function iterate(x, A, b, Binv, S)\n",
    "    \n",
    "    # println(x)\n",
    "    return x - Binv*A'*S*pinv(S'*A*Binv*A'*S)*S'*(A*x - b) # ' is the transpose operator, it has higher priority than *\n",
    "   \n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function RandomLinearSolve(x, A, b, Binv, sampling, T, skip)\n",
    "    \n",
    "    (m,n) = size(A)\n",
    "    xs = zeros(n, floor(Integer, T/skip) + 1) # will remember the iterates x here\n",
    "    fv = zeros(floor(Integer, T/skip) + 1)    # will remember residuals ||Ax-b|| here\n",
    "    \n",
    "    tic()\n",
    "    time = 0\n",
    "    \n",
    "    for t=0:T\n",
    "        tic()\n",
    "        x = iterate(x, A, b, Binv, sampling()) # notice that the last argument is a function\n",
    "        time = time + toq()\n",
    "        if t % skip == 0\n",
    "            xs[:,round(Int,t/skip)+1] = x           # remember iterate x\n",
    "            fv[round(Int,t/skip)+1] = norm(A*x-b)   # remeber residual \n",
    "            println(\"iteration: $(t), residual: $(fv[round(Int,t/skip+1)]) \")\n",
    "        end\n",
    "    end\n",
    "        \n",
    "    println(\"Time = \", time)\n",
    "    return xs,fv\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling\n",
    "\n",
    "We now write a function which outputs a random matrix $S$ (\"sampling/sketching\") which is equal to a random coordinate vector in $\\mathbb{R}^m$ chosen uniformly at random.\n",
    "\n",
    "${\\bf Problem:}$ You may want to modify the $\\verb\"sampling()\"$ function so that the probabilities are proportional to the squared norms of the rows of $A$ -- as initially proposed by Strohmer and Vershynin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition sampling() in module Main at In[37]:3 overwritten at In[42]:3.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sampling (generic function with 1 method)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function sampling() # for non-uniform sampling, looks into the function \"sample\" in the package StatsBase\n",
    "    \n",
    "    S = zeros(m)\n",
    "    S[rand(1:m)] = 1 # rand(range) returns a random number in the given range\n",
    "    return S\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition PlotResults(Any, Any, Any, Any) in module Main at In[38]:5 overwritten at In[43]:5.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PlotResults (generic function with 1 method)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyPlot\n",
    "\n",
    "function PlotResults(B, x_star, xs, fv)\n",
    "    \n",
    "    ax = axes()\n",
    "    plt[:plot](skip*(0:length(fv)-1), fv, \"-\", linewidth=3.0, label=L\"||Ax - b||\")\n",
    "    plt[:plot](skip*(0:length(fv)-1), [sqrt((xs[:,i]-x_star)'*B*(xs[:,i]-x_star)) for i=1:length(fv)] , \":\", linewidth=3.0, label=L\"||x - x^*||_B\")\n",
    "    legend(loc=\"upper right\")\n",
    "    ylabel(\"error\", fontsize=20)\n",
    "    xlabel(\"iterations\")\n",
    "    ax[:set_yscale](\"log\")\n",
    "    plt[:show]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Solve the problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Method definition Prueba(Any) in module Main at In[46]:4 overwritten at In[47]:4.\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "syntax: missing separator in array expression",
     "output_type": "error",
     "traceback": [
      "syntax: missing separator in array expression",
      ""
     ]
    }
   ],
   "source": [
    "function Prueba(n)\n",
    "    # input n: size of the matrix n x x\n",
    "    # output A: adjacency matrix of the cycle graph\n",
    "    A = zeros(n, n)\n",
    "    for i = 1:(n-1)\n",
    "        A[i, n-i+1] = 1\n",
    "        A[i, i] = -1\n",
    "    end\n",
    "    A[n,n] = 1\n",
    "    A[n,1] = -1\n",
    "    return A\n",
    "end\n",
    "A = Prueba(10)\n",
    "x = [10,9,8,7,6,5,4,3,2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extra output for the consensus problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 0, residual: 18.110770276274835 \n",
      "Time = 0.008437991\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\n",
       "[10.0 0.0 … 0.0 0.0; 9.0 0.0 … 0.0 0.0; … ; 2.0 0.0 … 0.0 0.0; 1.0 0.0 … 0.0 0.0],\n",
       "\n",
       "[18.1108,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0  …  0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#x = zeros(n)  # initial iterate\n",
    "#x = c\n",
    "\n",
    "#Binv = eye(n) # matrix B is one of the 2 parameters of the method\n",
    "\n",
    "#T = 100*m       # no of iterations \n",
    "#skip = round(T/30)    # we shall remember x each \"skip\" number of iterations\n",
    "T = 1000\n",
    "skip =1000.0/30.0\n",
    "Binv = eye(10)\n",
    "A = Prueba(10)\n",
    "x = [10,9,8,7,6,5,4,3,2,1]\n",
    "b = [0,0,0,0,0,0,0,0,0,0]\n",
    "# Now we solve the problem\n",
    "xs, fv = RandomLinearSolve(x, A, b, Binv, sampling, T, skip)\n",
    "\n",
    "# Let us now plot the results\n",
    "#B = inv(Binv)\n",
    "#PlotResults(B, x_star, xs, fv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average to be found = 1.1\n",
      "vector found = [1.0,2.0,3.0,4.0,5.0,6.0,7.0,8.0,9.0,10.0]\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "DimensionMismatch(\"dimensions must match\")",
     "output_type": "error",
     "traceback": [
      "DimensionMismatch(\"dimensions must match\")",
      "",
      " in promote_shape(::Tuple{Base.OneTo{Int64}}, ::Tuple{Base.OneTo{Int64}}) at ./operators.jl:406",
      " in _elementwise(::Base.#-, ::Type{Float64}, ::Array{Float64,1}, ::Array{Float64,1}) at ./arraymath.jl:57",
      " in -(::Array{Float64,1}, ::Array{Float64,1}) at ./arraymath.jl:49"
     ]
    }
   ],
   "source": [
    "x_last = xs[:,size(xs)[2]]\n",
    "println(\"average to be found = \", sum(c)/n)\n",
    "println(\"vector found = \",x_last+c)\n",
    "println(\"error = \", norm(x_last+c-ones(n)*sum(c)/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem A\n",
    "\n",
    "Code up a dedicated randomized Kaczmarz solver. That is, do it in an efficient way so that one does not need to run the $\\verb\"iterate\"$ function. Clearly, this function is rather inefficient - this is because it is so generic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem B\n",
    "\n",
    "Assume that $A^\\top A$ is positive definite. In this case, we can set $B=A^\\top A$. Run the basic method with this choice of $B$, and with $S = e_i$ with probability $1/m$. How do the iterates look like? That is, how does $x_{k+1}$ differ from $x_k$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem C\n",
    "\n",
    "Now define $\\cal D$ as follows: \n",
    "\n",
    "$$S = I_{:C},$$ \n",
    "\n",
    "where $C$ is a random subset of $\\{1,2,\\dots,n\\}$ of fixed cardinality $\\tau$ chosen uniformly at random. Test the basic method for various choices of $\\tau$. \n",
    "\n",
    "Find a $3\\times 3$ matrix $A$ such that running the randomized Newton method with $\\tau=2$ is vastly (= as much as you want) better than running it with $\\tau=1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem D\n",
    "\n",
    "Play with variants of the basic method where $S$ is a random Gaussian vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem E\n",
    "\n",
    "Can you come up with some other interesting sketching matrix $S$ not covered in the lecture? When would you use it? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
