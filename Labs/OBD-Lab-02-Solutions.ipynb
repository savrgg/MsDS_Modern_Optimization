{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>  MATH11146: Modern Optimization Methods for Big Data Problems </center>\n",
    "\n",
    "<center> University of Edinburgh</center>\n",
    "\n",
    "<center>Lecturer: Peter Richtarik</center>\n",
    "\n",
    "<center>Tutors: Sona Galovicova, Filip Hanzely and Nicolas Loizou</center>\n",
    "\n",
    "##  <center>Lab 2: Introduction to Gradient descent methods</center>\n",
    "<center>(C) Filip Hanzely and Peter Richtarik </center>\n",
    "<center> 18.1.2017 </center>\n",
    "\n",
    "\n",
    "## 1. Unicode characters in Julia\n",
    "\n",
    "Julia supports unicode characters. For instance, this means that we can use some of the LaTeX symbols as variables. Example: Type \\beta and then hit TAB to get the character $\\beta$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Œ≤=[1,2,3]\n",
    "5*Œ≤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of unicode characters supported by Julia can be found in Julia documentation. Codig in Julia may be visually very interesting. For instance, you can produce the following code to count cups of beer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "üç∫=1\n",
    "üçª=2\n",
    "üç∫+üçª"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By  the standard Euclidean inner (dot) product of two vectors $\\mathbf{u} = (u_1,\\dots,u_n)$ and $\\mathbf{v}=(v_1,\\dots,v_n)$ from $\\mathbb{R}^n$ we mean the quantity  $$\\langle \\mathbf{u}, \\mathbf{v} \\rangle = \\mathbf{u} \\cdot \\mathbf{v}= \\mathbf{u}^\\top \\mathbf{v} = \\sum_{i=1}^n u_i v_i.$$ \n",
    "All of the above notations are used in specific contexts in various fields, which can be confusing.\n",
    "\n",
    "In order to compute this in Julia, you will most often want to do this via the operation <b>u $\\cdot$ v</b> than via <b>u'\\*v</b>. Both are valid Julia expressions for computing the inner product. However, while the output of <b>u $\\cdot$ v</b> is a scalar value, the output of <b>u'*v</b> is a $1\\times 1$ matrix. Be aware of this, as it can break your code sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: invalid character \"¬∑\"",
     "output_type": "error",
     "traceback": [
      "syntax: invalid character \"¬∑\"",
      ""
     ]
    }
   ],
   "source": [
    "v = [1,4,5]\n",
    "u = [2,6,1]\n",
    "print(u‚ãÖv)\n",
    "\n",
    "print(u'*v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient descent\n",
    "The goal of this lab is to form better intuition about how gradient descent works.  You can find some basic information in  slides from Lecture 1. \n",
    "\n",
    "Throughout the rest of the lab we would like to solve the following least-squares problem:\n",
    "$$\\mathrm{min}\\ f(x):=\\frac12 \\|Ax-b\\|^2 .$$\n",
    "\n",
    "This is an important prboblem to solve on its own, with a plethora of applications. At the same time, it isa  prototype of a convex optimization problem, and one can get understanding by seeing how various optimization algorithms work on this problem.\n",
    "\n",
    "Let $x^0$ be a starting point,  $\\{\\alpha^t\\}>0$ a sequence of stepsize parameters. Gradient descent (GD) is given by the following recursion:\n",
    "$$x^{t+1}=x^t -\\alpha^t \\nabla f(x^t).$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function f(x,A,b)\n",
    "    return norm(A*x-b)/2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A=reshape([0,1,1,0],2,2)\n",
    "x=[1,2]\n",
    "b=[5,5]\n",
    "f(x,A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be useful to have access to the gradient of $f$. **Complete the function in the cell below.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function gradf(x,A,b)\n",
    "    return A'*(A*x-b)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **write a code for Gradient Descent in the cell below**. It should return a matrix having the starting point as the first row, first iterate as the second row, ..., $k$-th iterate as the $(k+1)$-th row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function GradientDescent(‚àáf,A,b,Œ±,x0,k)\n",
    "#Œ± - vector of stepsizes, Œ±[t]=Œ±^t\n",
    "#x0 - starting point\n",
    "#k - number if iterations\n",
    "    x=ones(k+1,size(x0)[1])\n",
    "    x[1,:]=x0\n",
    "    for i=1:k\n",
    "        x[i+1,:]=x[i,:]-Œ±[i]*‚àáf(x[i,:],A,b)\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "function plot_decrease(x,x_star,A,b,f,k,col)\n",
    "    fs = zeros(k)\n",
    "    for t=1:k\n",
    "        fs[t] = f(x[t,:],A,b)\n",
    "    end\n",
    "    f_star=f(x_star,A,b)\n",
    "    semilogy(1:k, fs-f_star, color=col) # plot on graph with logarithmic y-axis\n",
    "    xlabel(L\"t\")\n",
    "    ylabel(L\"f(x^{(t)})-f(x^*)\")\n",
    "    title(\"Convergence of the functional value\")\n",
    "    grid(\"on\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Testing Gradient Descent on a simple problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put everything together and run the gradient descent for some simple choice of $A,\\, b$ to check whether it actually works. Try to run it for **different choices of $\\alpha$** to see how the stepsize choice influences the convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# form a 3-by-3 diagonal matrix with values 2, 1, 3 on the diagonal\n",
    "A=diagm([2,1,3])\n",
    "\n",
    "b=[2,1,3]\n",
    "\n",
    "# it is easy to see that the solution of the optimization problem is x=[1,1,1]\n",
    "x_star=[1,1,1]\n",
    "\n",
    "# setting the starting point\n",
    "x0=[10,3,8]\n",
    " \n",
    "# number of iterations\n",
    "k=100\n",
    "\n",
    "#Setting stepsize. Should be less than 1/Œª_max(A'*A)\n",
    "Œ±=0.01*ones(1,k)\n",
    "\n",
    "\n",
    "x=GradientDescent(gradf,A,b,Œ±,x0,k)\n",
    "\n",
    "plot_decrease(x,x_star,A,b,f,k,\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Gradient Descent on a more complicated problem\n",
    "\n",
    "Now let's run gradient descent on some more complicated problem. \n",
    "\n",
    "We are going to generate a random matrix $A$. We would like to have $A^{\\top}A$ (hessian of $f$) to be positive definite and have the control of its condition number, i.e. \n",
    "$$\\frac{\\lambda_{\\mathrm{max}}(A^{\\top}A)}{\\lambda_{\\mathrm{min}}(A^{\\top}A)}=\n",
    "\\frac{\\sigma_{\\mathrm{max}}(A)}{\\sigma_{\\mathrm{min}}(A)}$$\n",
    "From singular value decomposition, we will generate $A$ as product $UDV$ where $U,\\, V$ are orthogonal matrices and $D=\\mathrm{diag}(\\sigma_1,\\dots, \\sigma_n)$. \n",
    "\n",
    "**Play with the condition number $\\kappa$ and stepsize parameter $\\alpha$ to see how they influence the convergence of the algorithm. **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dimension of the problem\n",
    "d=3\n",
    "#conditional numbar of hessian. Play with it and see what happens.  \n",
    "Œ∫=10\n",
    "#minimal eigenvalue of the hessian\n",
    "Œª_min=1\n",
    "\n",
    "#generate U,V as orthogonal matrices\n",
    "U=qr(rand(d,d))[1]\n",
    "V=qr(rand(d,d))[1]\n",
    "\n",
    "#generate D \n",
    "vec_D=vec(vcat([sqrt(Œª_min*Œ∫)],sqrt(Œª_min).+rand(d-2,1)*(sqrt(Œª_min*Œ∫)-sqrt(Œª_min)),[sqrt(Œª_min)]))\n",
    "D=diagm(vec_D)\n",
    "\n",
    "#get A from SVD\n",
    "A=U*D*V\n",
    "\n",
    "#chceck condition number of hessian (is should be close to Œ∫)\n",
    "println(\"cond(A'*A)= \",cond(A'*A))\n",
    "\n",
    "#random initialization of b\n",
    "b=randn(d,1)\n",
    "\n",
    "#random initialization of x0\n",
    "x0=randn(d,1)\n",
    "\n",
    "# number of iterations\n",
    "k=100\n",
    "\n",
    "# Œ± should be less than 1/Œª_max(A'*A). Play with it and see what happens \n",
    "Œ±=0.01*ones(1,k)\n",
    "\n",
    "#running gradient descent\n",
    "x=GradientDescent(gradf,A,b,Œ±,x0,k)\n",
    "\n",
    "#getting the exact solution\n",
    "x_star=\\(A,b)\n",
    "\n",
    "#plotting the results\n",
    "plot_decrease(x,x_star,A,b,f,k,\"blue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Projected Gradient Descent\n",
    "Projected Gradient Descent is used to minimize function $f$ over a set $Q \\in \\mathbb{R}^n$. The iterations are given by\n",
    "\n",
    "$$ x^{t+1}=\\Pi_Q\\big(x^t-\\alpha^t\\nabla f(x^t)\\big), \\qquad \\Pi_Q(y)=\\mathrm{argmin}_{z\\in Q}\\|y-z \\|^2 $$\n",
    "    \n",
    "Above, $\\Pi_Q(y)$ is the Euclidean projection of $y$ onto the set $Q$.\n",
    " \n",
    "Your task is to **write the code of Projected Gradient Descent  in the cell below**. Use the constraint  $Q=\\{x \\;:\\; \\|x\\|^2=1 \\}$ (unit ball). First, you will need to figure out on a piece of paper what the formula for $\\Pi_{Q}(y)$ is. \n",
    "\n",
    "If you are fast with this, play with some other constraints, such as $Q = \\{x\\;:\\; a^\\top x = 1\\}$ for some vector $a\\in \\mathbb{R}^n$.\n",
    "\n",
    "As before, the code should return a matrix having the starting point as the first row, first iterate as the second row, ..., $k$-th iterate as the $(k+1)$-th row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function ProjGD(‚àáf,A,b,Œ±,x0,k)\n",
    "#Œ± - vector of stepsizes, Œ±[t]=Œ±^t\n",
    "#x0 - starting point\n",
    "#k - number if iterations\n",
    "    x=ones(k+1,size(A)[1])\n",
    "    x[1,:]=x0\n",
    "    for i=1:k\n",
    "        xc=x[i,:]-Œ±[i]*‚àáf(x[i,:],A,b)\n",
    "        x[i+1,:]=xc/(norm(xc))\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have implemented all neccessary tools to test the Projected Gradient Method. Try to run the code below for **different choices of $\\alpha$** to see how the stepsizes influence the convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A=eye(3)\n",
    "b=[4,4,4]\n",
    "#Note that the solution is x=[1,1,1]\n",
    "x_star=[ 1/sqrt(3), 1/sqrt(3), 1/sqrt(3)]\n",
    "\n",
    "#setting the starting point\n",
    "x0=[-1/sqrt(2),-1/sqrt(2),0]\n",
    " \n",
    "#number of iterations\n",
    "k=30\n",
    "\n",
    "##Setting stepsize. Good choice can be 1/Œª_max(A'*A)\n",
    "Œ±=100*ones(1,k)\n",
    "\n",
    "x=ProjGD(gradf,A,b,Œ±,x0,k)\n",
    "\n",
    "plot_decrease(x,x_star,A,b,f,k,\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Accelerated Gradient Descent\n",
    "\n",
    "Accelerated Gradient Descent (AGD) is an very fast method for minimizing a smooth and strongly convex function.  \n",
    "The iterations are given recursively by\n",
    "    $$y^{t+1}=x^t-\\alpha^t\\nabla f(x^t), \\qquad x^{k+1}=(1+\\beta^t)y^{t+1}-\\beta^ty^t .$$\n",
    " \n",
    "Your task is to **write the code of Accelerated Gradient Descent algorithm** in the cell below. It should return a matrix having the starting point as the first row, first iterate as the second row, ..., $k$-th iterate as the $(k+1)$-th row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "function AccGD(‚àáf,A,b,Œ±,Œ≤,x0,k)\n",
    "#Œ± - stepsize parameter vector\n",
    "#Œ≤ - stepsize parameter vector\n",
    "#x0 - starting point\n",
    "#k - number if iterations\n",
    "    x=ones(k+1,size(x0)[1])\n",
    "    x[1,:]=x0\n",
    "    y=ones(k+1,size(x0)[1])\n",
    "    y[1,:]=x0\n",
    "    for i=1:k\n",
    "        y[i+1,:]=x[i,:]-Œ±[i]*‚àáf(x[i,:],A,b)\n",
    "        x[i+1,:]=(1+Œ≤[i])*y[i+1,:]-Œ≤[i]*y[i,:] \n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to run the code below for **different choices of $\\alpha$, $\\beta$ and $\\kappa$** to to see how the stepsize and condition number influence the convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dimension of the problem\n",
    "d=100\n",
    "#conditional numbar of hessian. Play with it and see what happens.  \n",
    "Œ∫=50\n",
    "\n",
    "#minimal eigenvalue of the hessian\n",
    "Œª_min=1\n",
    "\n",
    "#getting A from SVD decomposition\n",
    "#generate U,V as orthogonal matrices\n",
    "U=qr(rand(d,d))[1]\n",
    "V=qr(rand(d,d))[1]\n",
    "\n",
    "#generate D \n",
    "vec_D=vec(vcat([sqrt(Œª_min*Œ∫)],sqrt(Œª_min).+rand(d-2,1)*(sqrt(Œª_min*Œ∫)-sqrt(Œª_min)),[sqrt(Œª_min)]))\n",
    "D=diagm(vec_D)\n",
    "\n",
    "\n",
    "#get A from SVD\n",
    "A=U*D*V\n",
    "\n",
    "#chceck condition number of hessian \n",
    "println(\"cond(A'*A)= \",cond(A'*A))\n",
    "\n",
    "#random initialization of b\n",
    "b=randn(d,1)\n",
    "\n",
    "#random initialization of x0\n",
    "x0=randn(d,1)\n",
    "\n",
    "# number of iterations\n",
    "k=1000\n",
    "\n",
    "# Play with Œ± to get a better intuition about the algorithm. Good choice can be 1/(Œ∫*Œª_min)*ones(1,n)\n",
    "Œ±=0.01*ones(1,k)\n",
    "# Play with Œ≤ to get a better intuition about the algorithm. Good choice can be (sqrt(Œ∫)-1)/(sqrt(Œ∫+1))*ones(1,n)\n",
    "Œ≤=0.5*(sqrt(Œ∫)-1)/(sqrt(Œ∫+1))*ones(1,k)\n",
    "\n",
    "\n",
    "#running accelerated gradient descent\n",
    "x=AccGD(gradf,A,b,Œ±,Œ≤,x0,k)\n",
    "\n",
    "#getting the exact solution\n",
    "x_star=\\(A,b)\n",
    "\n",
    "#plotting the results\n",
    "plot_decrease(x,x_star,A,b,f,k,\"blue\")\n",
    "\n",
    "#comparing with gradient descent\n",
    "x_gd=GradientDescent(gradf,A,b,Œ±,x0,k)\n",
    "plot_decrease(x_gd,x_star,A,b,f,k,\"red\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
