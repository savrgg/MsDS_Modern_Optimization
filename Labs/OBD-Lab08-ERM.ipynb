{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>  MATH11146: Modern Optimization Methods for Big Data Problems </center>\n",
    "\n",
    "<center> University of Edinburgh</center>\n",
    "\n",
    "<center>Lecturer: Peter Richtarik</center>\n",
    "\n",
    "<center>Tutors: Sona Galovicova, Filip Hanzely, Jakub Konecny</center>\n",
    "\n",
    "##  <center>Lab 8: Empirical Risk Minimization</center>\n",
    "<center>(C) Dominik Csiba and Peter Richtarik </center>\n",
    "<center> 15.2.2017 </center>\n",
    "\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The purpose of this lab is to fool around with an efficient randomized algorithm for <i>minimizing finite sums</i>. In particular, we will be solving the L2-regularized empirical risk minimization (ERM) problem, namely \n",
    "\n",
    "$$\\min_{w\\in \\mathbb{R}^d} \\left\\{ P(w) \\equiv \\frac{1}{n}\\sum_{i=1}^n \\phi_i(X_i^T w) + \\frac{\\lambda}{2}\\|w\\|_2^2 \\right\\},$$\n",
    "using Dual-Free SDCA (dfSDCA) [1, 2]. \n",
    "\n",
    "The meaning of the terms above:\n",
    "- $w\\in \\mathbb{R}^d$ is a linear predictor,\n",
    "- $X_1,\\dots,X_n \\in \\mathbb{R}^{m\\times d}$ are training examples (in this code we will assume that $m=1$ and hence the examples are simply just vectors),\n",
    "- $\\phi_i:\\mathbb{R}^d \\to \\mathbb{R}$ is the loss incurred on example $i$ (label associated with example $i$ is already incorporated in the definition of $\\phi_i$; this why the function $\\phi_i$ has a subscript),\n",
    "- $\\lambda>0$ is a regularization parameter (typically, $\\lambda=\\frac{1}{n}$),\n",
    "- $P$ is the regularized empirical risk. \n",
    "\n",
    "\n",
    "$\\textbf{Assumption 1.}$ The loss functions $\\phi_i:\\mathbb{R}^m\\mapsto \\mathbb{R}$, $i=1,\\dots,n$, are $\\tfrac{1}{\\gamma}$-smooth. That is, for all $u,v\\in \\mathbb{R}^m$ the following inequality holds:\n",
    "$$\\| \\nabla \\phi_i(u) - \\nabla \\phi_i(v) \\|_2 \\leq \\frac{1}{\\gamma}\\|u-v\\|_2.$$\n",
    "This is equivalent to requiring that for all $u,v\\in \\mathbb{R}^m$ one has\n",
    "$$ \\phi_i(u+v) \\leq \\phi_i(u) + (\\nabla \\phi_i(u))^\\top v + \\frac{1}{2\\gamma}\\|v\\|_2^2.$$\n",
    "\n",
    "$\\textbf{Assumption 2.}$  The loss functions $\\phi_i:\\mathbb{R}^m \\mapsto \\mathbb{R}$, $i=1,\\dots,n$, are convex. That is, for all $u,v\\in \\mathbb{R}^m$:\n",
    "\n",
    "$$\\phi(u) + (\\nabla \\phi_i(u))^\\top v \\leq \\phi_i(u+v).$$\n",
    "\n",
    "The standard SDCA (Stochastic Dual Coordinate Ascent) algorithm, as its name indicates, can be interreted as applying Stochastic/Randomized Coordinate Descent [3] to the Fenchel dual of the ERM problem. The dual problem is naturaly a concave maximization problem, and hence the word \"ascent\" is used instead of \"descent\". However, we have seen that dfSDCA is <i>not</i> motivated nor analyzed via the dual problem, and in this sense, the method is <i>dual-free</i>.\n",
    "\n",
    "- [1] S. Shalev-Shwartz. SDCA without duality, <i>arXiv:1502.06177</i>, 2015\n",
    "- [2] D. Csiba and P. Richtarik. Primal method for ERM with flexible mini-batching schemes and non-convex losses,\n",
    "<i>arXiv:1506.02227</i>, 2015\n",
    "- [3] P. Richtarik and M. Takac. Iteration complexity of randomized block-coordinate descent methods for minimizing a composite function, <i>Mathematical Programming</i> 144(2):1-38, 2014 (arXiv:1107.2848)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preliminary Stuff\n",
    "\n",
    "### 2.1  Load a package for plotting \n",
    "\n",
    "We first load a package for plotting: <b>PyPlot</b>.\n",
    "\n",
    "### 2.2  Load a package for sampling from a probability distribution\n",
    "\n",
    "We then include the  package <b>probability_tree.jl</b> with functions that will be useful for sampling from a non-uniform distribution from a finite set $[n]:=\\{1, \\dots, n\\}$. In the lecture, we have considered <i>arbitrary samplings</i>, i.e., arbitrary set-valued mappings with values being the subsets of $[n]$. However, the package <b>probability_tree.jl</b> only implements samplings of  the form\n",
    "\n",
    "$$\\mathbf{P}(S_t = \\{i\\}) = p_i,$$\n",
    "\n",
    "where $p_i>0$ for all $i\\in [n]$ and $\\sum_{i=1}^n p_i = 1$. In particular, the package <b>probability_tree.jl</b> contains two  routines:\n",
    "- <b>PTCreate</b> - with input being the weights $p$ and output being an array which we call a probability tree, \n",
    "- <b>PTSample</b> - with inpit being a probability tree and and output being a random integer $i$ with probability $p_i$.\n",
    "\n",
    "### 2.3  Load a package for loading data from an external file\n",
    "\n",
    "Finally, the package <b>load_matricies.jl</b> contains two routines:\n",
    "- <b>ReadLIBSVM</b> - reads a dataset in LIBSVM format, inputs the filename, shape and a boolean <i>classification</i> indicating whether the dataset is for regression or classification, and outputs the matrix and labels\n",
    "- <b>ReadData</b> - reads a dataset in a standard format, inputs the filename and a boolean <i>classification</i> indicating whether the dataset is for regression or classification, and outputs the matrix and labels\n",
    "\n",
    "The data has to be stored in the folder \"data\".\n",
    "\n",
    "### 2.4  Operator overloading\n",
    "\n",
    "We will also overload the $\\cdot$ operator so that it outputs a scalar as a result of an inner product of a vecor represented as a sparse matrix and a vector represented as an array. These operations will be used often. The default type of the result, without the operator overloading, is a sparse matrix, which would be inconvenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Loading help data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dot (generic function with 9 methods)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using PyPlot\n",
    "\n",
    "include(\"probability_tree.jl\")\n",
    "\n",
    "include(\"load_matrices.jl\")\n",
    "\n",
    "⋅(x::SparseMatrixCSC{Float64, Int64}, y::Array{Float64,1}) = (x'*y)[1]\n",
    "⋅(x::SparseMatrixCSC{Float64, Int64}, y::SparseMatrixCSC{Float64, Int64}) = (x'*y)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Functions\n",
    "\n",
    "We assume that the $n$ examples $X_1,\\dots,X_n \\in \\mathbb{R}^d$ are stored as columns of a $d\\times n$ matrix $X$ (technically, a \"sparse matrix\" in Julia). We also assume that the associated labels $y_1,\\dots,y_n\\in \\mathbb{R}$ are stored as entries of a vector  $y\\in \\mathbb{R}^n$ (technically, an \"array\" in Julia).\n",
    "\n",
    "In particular, we work with the quadratic loss:\n",
    "\n",
    "$$ \\phi_i(t) = \\frac{1}{2\\gamma} (t - y_i)^2, \\qquad  \\nabla \\phi_i(t) = \\frac{1}{\\gamma} (t-y_i), \\qquad \\phi_i(X_i^\\top w) = \\frac{1}{2\\gamma} (X_{i}^\\top w - y_i)^2, \\qquad \\nabla \\phi_i(X_i^\\top w) = \\frac{1}{\\gamma} (X_i^\\top w - y_i).$$\n",
    "\n",
    "It can be verified that $\\phi_i$ is indeed $\\tfrac{1}{\\gamma}$-smooth and convex. \n",
    "\n",
    "We now define a function <b>Loss_quadratic</b> whose output will be a touple:\n",
    "- function <b>P(w)</b> \n",
    "- function <b>g(w,i)</b> (which evaluates $\\nabla \\phi_i(X_i^\\top w)$). \n",
    "\n",
    "Indeed, functions are first-class citizens in Julia. They can be an output of other functions. This is great, since it will allow us to write a single dfSDCA code which will call generic functions $P$ and $g$. Whenever we change these functions, the code runs with these new functions without any changes. This makes it easy to write flexible optimization code in Julia.\n",
    "\n",
    "\n",
    "### Exercise A: Logistic Regression\n",
    "\n",
    "Replace the three ??? signs in the <b>Loss_logistic</b> function below by suitable code so that the  function works analogously to the <b>Loss_quadratic</b> function.  \n",
    "\n",
    "<i> Hint: </i> The logistic loss is defined as\n",
    "\n",
    "$$ \\phi_i(t) = \\frac{4}{\\gamma} \\log \\left(1 + e^{y_i t} \\right)$$\n",
    "\n",
    "and its derivative is\n",
    "\n",
    "$$\\nabla \\phi_i(t) = \\frac{4 y_i}{\\gamma \\left(1+ e^{-y_i t}\\right)} \\enspace .$$\n",
    "\n",
    "Again, it can be verified that $\\phi_i$ is indeed $\\tfrac{1}{\\gamma}$-smooth and convex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: colon expected in \"?\" expression\nwhile loading In[2], in expression starting on line 12",
     "output_type": "error",
     "traceback": [
      "syntax: colon expected in \"?\" expression\nwhile loading In[2], in expression starting on line 12",
      ""
     ]
    }
   ],
   "source": [
    "function Loss_quadratic(X::SparseMatrixCSC, y::Array{Float64,1}, γ::Float64, λ::Float64)\n",
    "    n = size(X)[2]\n",
    "    f(w, i) = 1/(2γ)*(X[:,i]⋅w - y[i])^2 # the overloading is used here\n",
    "    g(w, i) = 1/γ*(X[:,i]⋅w - y[i]) # and here\n",
    "    P(w)    = 1/n*sum([f(w,i) for i=1:n]) + λ/2*w⋅w\n",
    "    return (g, P)\n",
    "end\n",
    "\n",
    "function Loss_logistic(X::SparseMatrixCSC, y::Array{Float64,1}, γ::Float64, λ::Float64)\n",
    "    n = size(X)[2]\n",
    "    f(w, i) = ???\n",
    "    g(w, i) = ???\n",
    "    P(w)    = ???\n",
    "    return(g, P)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The dfSDCA Algorithm\n",
    "\n",
    "Recall that the dfSDCA algorithm looks as follows:\n",
    "\n",
    "Initialize:\n",
    "- Choose a sampling (random set-valued mapping) $\\hat{S}$ \n",
    "- Compute $p_i = \\mathbf{P}(i \\in \\hat{S})$\n",
    "- Compute \"stepsize\" parameters $v_1,\\dots,v_n$ satisfying the ESO inequality\n",
    "$$P \\circ X^\\top X \\preceq Diag(p \\circ v),$$\n",
    "where $v = (v_1,\\dots,v_n)$, $p=(p_1,\\dots,p_n)$ and $P$ is the $n\\times n$ \"probability\" matrix associated with $\\hat{S}$. That is, $P$ is the matrix with $(i,j)$ entry equal to $\\mathbf{P}(\\{i,j\\}\\subseteq \\hat{S})$. Note that we are now using $P$ both for the probability matrix and the primal objective function. This is too bad, but then no confusion should be caused by this! This form of ESO only makes sense if $m=1$; for $m>1$, the correct ESO inequality looks a bit different [2]. \n",
    "- Set \n",
    "$$\\theta =  \\min_{i}  \\frac{p_i \\lambda \\gamma n}{v_i + \\lambda \\gamma n}$$\n",
    "- Choose initial \"dual\" variables $\\alpha_1^{0}, \\dots, \\alpha_n^{0} \\in \\mathbb{R}^m$\n",
    "- Set \n",
    "$$w^{0} = \\frac{1}{\\lambda n}\\sum_{i=1}^n X_i\\alpha_i^{0}$$\n",
    "\n",
    "For $t \\geq 0$ repeat:\n",
    "- Choose a random set (minibatch) $S_t\\subseteq \\{1,2,\\dots,n\\}$ of examples, with $S_t\\sim \\hat{S}$ \n",
    "- For $i \\in S_t$ do (possibly in parallel): \n",
    "$$\\alpha_i^{t+1} = \\left( 1- \\frac{\\theta}{p_i} \\right) \\alpha_i^{t} + \\frac{\\theta}{p_i}(-\\nabla\\phi_i(X_i^\\top w^{t}))$$\n",
    "- Update the main variable:\n",
    "$$w^{t+1} = w^{t} - \\sum_{i\\in S_t} \\frac{\\theta}{\\lambda n p_i}  X_i(\\nabla\\phi_i(X_i^\\top w^{t}) + \\alpha_i^{t})$$\n",
    "\n",
    "\n",
    "Now we write a Julia function called <b>dfSDCA</b> which implements the algorithm in the special case when $m=1$ and $|\\hat{S}|=1$ with probability 1. It will have the following input variables:\n",
    "- $X$ = $d\\times n$ data matrix\n",
    "- $g$ = function evaluating $\\phi_i(X_i^\\top w)$\n",
    "- $P$ = the objective function\n",
    "- $p$ = an array representing a discrete probability distribution over $\\{1,2,\\dots,n \\}$\n",
    "- $T$ = number of iterations\n",
    "- $\\gamma$ = smoothness parameter (we assume $\\phi_i$ is $1/\\gamma$-smooth)\n",
    "- $\\lambda$ = strong-convexity parameter\n",
    "- <i>track</i> = a boolean deciding whether we want to output function values or the primal variable $w^{T}$\n",
    "- <i>AlgLabel</i> = label for the print outputs during the run of the algorithm\n",
    "- <i>progress</i> = a boolean value indicating whether the progress is shown in the console or not\n",
    "\n",
    "### Exercise B\n",
    "\n",
    "There are 5 places with questions marks ??? in the dfSDCA code below. Replace them with the correct code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: colon expected in \"?\" expression\nwhile loading In[3], in expression starting on line 11",
     "output_type": "error",
     "traceback": [
      "syntax: colon expected in \"?\" expression\nwhile loading In[3], in expression starting on line 11",
      ""
     ]
    }
   ],
   "source": [
    "function dfSDCA(X::SparseMatrixCSC, g::Function, P::Function, p::Array{Float64,1}, T::Int64, \n",
    "    γ::Float64, λ::Float64, track::Bool, AlgLabel::ASCIIString, progress::Bool)\n",
    "    \n",
    "(d,n) = size(X)\n",
    "    \n",
    "if track\n",
    "        Pvalue = zeros(ifloor(T/n)+1) # prepares the array of function values\n",
    "end\n",
    "     \n",
    "v = ???\n",
    "\n",
    "θ = minimum(p*λ*γ*n./(v + λ*γ*n)) # step size\n",
    "\n",
    "PT = ??? # creates the probability tree\n",
    "\n",
    "α = zeros(n) # the starting points are set to zero vectors for simplicity\n",
    "w = ???\n",
    "\n",
    "for t=0:T\n",
    "\n",
    "    if t % n == 0 # every n iterations store the function value and prints a message about progress\n",
    "        if track\n",
    "            Pvalue[t/n + 1] = P(w)\n",
    "            if progress\n",
    "                println(AlgLabel, \":  #passes $(t/n), function value: $(Pvalue[t/n + 1])\")\n",
    "            end\n",
    "        else\n",
    "            if progress\n",
    "                println(AlgLabel, \":  #passes $(t/n)\")\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    i = PTSample(PT) # sample a coordinate according to distribution p\n",
    "    Δ = g(w,i) + α[i] # store the update as Δ\n",
    "    α[i] -= ???\n",
    "    w -= (X[:,i]*θ*Δ) / (???) # vector - matrix = matrix\n",
    "    w = vec(w) # matrix -> vector \n",
    "end\n",
    "\n",
    "println( AlgLabel ,\" finished!\" )\n",
    "\n",
    "if track # return function values or w, dependig on track\n",
    "    return Pvalue\n",
    "else\n",
    "    return w\n",
    "end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Plotting Functions\n",
    "\n",
    "We now write 2 functions which will be used to visualize the results. \n",
    "\n",
    "Function <b>ShowConvergence</b> takes the following input:\n",
    "- <i>Pvalues</i> - a tuple of function value arrays\n",
    "- <i>AlgLabels</i> - a tuple of algorithm labels, for legend\n",
    "- <i>Pstar</i> - the optimal function value\n",
    "\n",
    "Function <b>dfSDCACompare</b> is used for comparing the behavior of several variants of <b>dfSDCA</b> in a single plot. The variants differ in the choice of the probability vector $p$. The function first finds the optimal function value (by running the <b>dfSDCA</b> method for 60 passes over data), and then uses the objective function value found in lieu of the true minimum. This is used in order to plot $P(w^t) - P(w^*)$  on the $y$ axis in logarithmic scale.\n",
    "\n",
    "Input:\n",
    "\n",
    "- $X$ = data matrix\n",
    "- $g$ = function evaluating $\\phi_i(X_i^\\top w)$\n",
    "- $P$ = the primal objective function\n",
    "- $T$ = number of iterations\n",
    "- $\\gamma$ = smoothness parameter\n",
    "- $\\lambda$ = strong-convexity parameter\n",
    "- $plist$ = a tuple of probability distributions\n",
    "- <i>AlgLabel</i> = tuple of labels for the print outputs during the run of the algorithm and for the legend\n",
    "- <i>progress</i> = a boolean indicating, whether the progress is shown in the console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dfSDCACompare (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function ShowConvergence(Pvalues, AlgLabels, Pstar::Float64)    \n",
    "    ax = axes()\n",
    "    for i=1:length(Pvalues)\n",
    "        plt[:plot](0:(length(Pvalues[i])-1), abs(Pvalues[i]-Pstar), \"-\", linewidth=3.0, label=AlgLabels[i])\n",
    "    end\n",
    "    legend(loc=\"upper right\")\n",
    "    ylabel(L\"$P(w^{t}) - P(w^\\star)$\", fontsize=20)\n",
    "    xlabel(\"passes over data\")\n",
    "    ax[:set_yscale](\"log\")\n",
    "    plt[:show]()\n",
    "end\n",
    "\n",
    "function dfSDCACompare(X::SparseMatrixCSC, g::Function, P::Function, \n",
    "    T::Int64, γ::Float64, λ::Float64, plist, AlgLabel, progress::Bool)\n",
    "    v = float([X[:,i]⋅X[:,i] for i=1:n])\n",
    "    p_imp = v + λ*γ*n\n",
    "    p_imp = p_imp/sum(p_imp)\n",
    "    wstar = dfSDCA(X, g, P, p_imp, 2T, γ, λ, false, \"Optimal value\", progress)\n",
    "    Pstar = P(wstar)\n",
    "    Pvalues = [dfSDCA(X, g, P, plist[i], T, γ, λ, true, AlgLabel[i], progress) for i = 1:length(plist)]\n",
    "    ShowConvergence(Pvalues, AlgLabel, Pstar)\n",
    "end "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Run the Method on a Problem with Synthetic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1  We first generate a random problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(g,P)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10000\n",
    "d = 100\n",
    "X = sprand(d, n, 0.1)\n",
    "y = rand(n)\n",
    "γ = 1.0\n",
    "λ = 1/n\n",
    "g, P = Loss_quadratic(X,y,γ,λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2  Now we define three different probability distributions\n",
    "\n",
    "These will correspond to three different versions of the dfSDCA method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000-element Array{Float64,1}:\n",
       " 4.97974e-5 \n",
       " 6.58846e-5 \n",
       " 8.66007e-5 \n",
       " 7.495e-5   \n",
       " 2.66471e-5 \n",
       " 0.000183275\n",
       " 0.000123811\n",
       " 0.000181004\n",
       " 9.84303e-5 \n",
       " 0.000115136\n",
       " 0.000124783\n",
       " 6.58301e-5 \n",
       " 0.00015778 \n",
       " ⋮          \n",
       " 2.87644e-5 \n",
       " 0.000120025\n",
       " 0.000133289\n",
       " 6.73098e-5 \n",
       " 0.000174559\n",
       " 0.000113464\n",
       " 0.000132749\n",
       " 9.90225e-5 \n",
       " 2.86598e-5 \n",
       " 8.34707e-5 \n",
       " 7.01152e-5 \n",
       " 1.73417e-5 "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p1 = ones(n)/n\n",
    "v = float([X[:,i]⋅X[:,i] for i=1:n]) # julia is having difficulties with arrays of type \"Any\"\n",
    "p2 = v + n*λ*γ\n",
    "p2 = p2/sum(p2)\n",
    "p3 = 0.1 + rand(n)\n",
    "p3 = p3/sum(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3  We can now run the method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "dfSDCA not defined\nwhile loading In[7], in expression starting on line 1",
     "output_type": "error",
     "traceback": [
      "dfSDCA not defined\nwhile loading In[7], in expression starting on line 1",
      "",
      " in dfSDCACompare at In[4]:18"
     ]
    }
   ],
   "source": [
    "dfSDCACompare(X, g, P, 30n, γ, λ, (p1,p2,p3), (\"dfSDCA-unif\", \"dfSDCA-imp\", \"dfSDCA-rand\"), false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run the Method on a Problem with Real Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "Loss_logistic not defined\nwhile loading In[8], in expression starting on line 7",
     "output_type": "error",
     "traceback": [
      "Loss_logistic not defined\nwhile loading In[8], in expression starting on line 7",
      ""
     ]
    }
   ],
   "source": [
    "X,y = ReadData(\"mushrooms\", true)\n",
    "#X,y = ReadLIBSVM(\"ijcnn1.tr\", (49990,22) ,true)\n",
    "\n",
    "d,n = size(X)\n",
    "γ = 1.0\n",
    "λ = 1/n\n",
    "g, P = Loss_logistic(X,y,γ,λ)\n",
    "\n",
    "p1 = ones(n)/n\n",
    "v = float([X[:,i]⋅X[:,i] for i=1:n])\n",
    "p2 = v + λ*γ*n\n",
    "p2 = p2/sum(p2)\n",
    "p3 = 0.1 + rand(n)\n",
    "p3 = p3/sum(p3)\n",
    "\n",
    "dfSDCACompare(X, g, P, 30n, γ, λ, (p1,p2,p3), (\"dfSDCA-unif\", \"dfSDCA-imp\", \"dfSDCA-rand\"), true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Further Exercises\n",
    "\n",
    "### Exercise C\n",
    "\n",
    "Write a new function, <b>dfSDCA10</b>, which always updates a set of $10$ variables $\\alpha_i$, chosen uniformly at random. That is, $|S_t|=10$ with probability 1, and all such subsets of cardinality 10 are equaly likely chosen. Make sure the stepsize parameters $v$ are chosen correctly, following theoretical recommendation.\n",
    "\n",
    "### Exercise D\n",
    "\n",
    "Write a new function, <b>dfSDCAall</b>, which in each iteration updates all variables $\\alpha_i$. This method is deterministic. Compare its behavior with dfSDCA with uniform sampling of a single \"dual\" variable in a single plot.\n",
    "\n",
    "### Exercise E\n",
    "\n",
    "Code up the stochastic gradient descent algorithm (with a fixed stepsize). For a range of stepsizes, plot its behavior (as it is done for dfSDCA in the above plots). Compare with dfSDCA.\n",
    "\n",
    "### *Exercise F\n",
    "\n",
    "Does it make sense to update the probability vector $p$ throughout the iterations of dfSDCA? Can you come up with an adaptive strategy that leads to a faster method in runtime? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.5.0",
   "language": "julia",
   "name": "julia-0.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
